<img width="654" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/0aa1b446-c596-42ab-b65e-95f88274cb75">##  可视化
<img width="965" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/a6dd53bb-77eb-45fc-beca-43e13f7c62cd">


##标准化

归一化的目标
把数变为（0，1）之间的小数 主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。
把有量纲表达式变为无量纲表达式 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。

归一化的好处
1. 提升模型的收敛速度
2.提升模型的精度
3. 深度学习中数据归一化可以防止模型梯度爆炸。

常用的标准化方法
min-max标准化(min-max normalization)也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下:其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义
Min-Max Normalisation (use 5%/95% percentile as the min and max more robust to outliers)
X𝑛𝑜𝑟𝑚= (𝑋 − min(𝑋))/(max(X)-min(X))

o Z-Normalisation (or Zero-mean normalisation)
X𝑛𝑜𝑟𝑚= (𝑋 − μ)/σ
X is the feature vector of original values, μ and σ are the mean and standard deviation of vector X
当然并非所有数据标准化的结果都需要映射到[0,1]区间上，这时就可以使用z-score标准化方法，该方法是SPSS中最为常用的标准化方法:z-score 标准化(zero-mean normalization)也叫标准差标准化，该方法使得经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为:y=(x−μ)/σ，其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
Vector Normalisation
<img width="186" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/119a5160-e495-477d-9c10-d8150e52c66a">
| 𝑋 | is the vector length of X

数据输入（处理缺失数据）
数据估算方法：
- 使用平均值或中值
- 使用最频繁值
- 根据特征相似性使用 k 近邻法
- 通过链式方程使用多元估算（即尝试用不同的值多次填补缺失数据，并将最终结果汇集起来 
用不同的值多次填补缺失数据，并汇总最终结果）
- 使用基于其他特征的机器学习模型估计缺失值特征

## 维度灾难
  机器学习中维度的诅咒是什么？-数据样本在特征空间中过于稀疏。（即实例的数量不够大，不足以在特征空间中密集分布）

  我们该做些什么来避免对维度的诅咒呢？
  o增加数据样本的数量。所需的样本数将需要随着特征维数的线性增长而呈指数增长。
  o减少特征的数量（更可行）：使用特征选择和降维方法

## 特征选择和降维方法

FS和DR的目标
- 降低 "维度诅咒 "造成的影响
- 去除冗余特征以提高性能
- 提高计算效率
- 降低新数据采集成本
- 
FS 与 DR
FS 保留原始特征的子集
DR 生成新的特征集，该特征集结构紧凑，但保留了特征的原始含义。
原始特征的含义。

## 在使用FS和DR时需要考虑的事情
- 目标维度
- 可解释性（是：FS；否：博士或 FS）
- 特征相关性/依赖性
- 特征可靠性和可重复性
- 方法（不同的方法可能产生不同的特征

特征选择 
 过拟合的表现是模型参数太贴合训练集数据，模型在训练集上效果很好而在测试集上表现不好，也就是在高方差。简言之模型的泛化能力差。

 正则化 
1.如何防止模型过拟合？
  收集更多数据
  通过正则化引入对复杂度的惩罚
  选择更少参数的简单模型
  对数据降维（降维有两种方式：特征选择和特征抽取

2.<img width="654" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/bcbe954e-8a5d-4a7a-86b5-4a238ef324ff">

<img width="1195" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/01e75788-a89e-4bf7-a494-59312d4f4320">
<img width="912" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/0cfa36f5-7aa9-488e-9a23-2212d8e7b7b3">

## KKT 
<img width="657" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/fe92f2ec-26a2-4ba6-9518-29ba3eb54ac6">
Karush-Kuhn-Tucker (KKT)条件是非线性规划(nonlinear programming)最佳解的必要条件。KKT条件将Lagrange乘数法(Lagrange multipliers)所处理涉及等式的约束优化问题推广至不等式。

<img width="535" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/42f008e2-7d2b-4296-b203-e3aa54430732">
为啥L1 有稀疏性
 <img width="332" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/5a1fe3ff-66e8-44f0-b824-e96ff25d8af1">

 ## 特征选择
封装方法
o 寻找最优特征子集，使决策性能最大化 -- 跟后续的机器学习算法相关联  
o 方法：递归特征消除；顺序特征选择。
- 嵌入式方法
o 将 FS 过程整合到模型学习过程中。 -- 融在一起的
o 方法：脊（ElasticNet）；套索；随机森林（特征排序）。
- 基于筛选的方法 
o 根据特征关系和统计数据而非性能进行选择。
o 方法：单变量（方差分析）；Chi Square；相关/方差

## 卡方检验
<img width="442" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/5e92e316-5bbe-4b4a-a93d-363579fa270e">

<img width="610" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/6dec9600-3e73-4984-af7a-5a39ea2bbdd2">

<img width="624" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/a2424d76-7abc-4e31-82d8-e50a15f800e0">

<img width="432" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/19cc05c5-dcb6-4637-8257-0ef2af30122d">

<img width="535" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/e75bb281-70b3-41cf-a9fb-25eee6a14159">

## LASSO (Embedded Method)
<img width="954" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/4c2f1ece-3d00-46c7-92f3-b8f200803669">
- LASSO（最小绝对收缩和选择算子）
- 添加 L1 正则项，以减少有效特征的数量
- 损失函数不可微 子梯度法或最小角度 回归来优化损失函数

皮尔逊相关系数( r )是测量线性相关性的最常见方法。它是一个介于 –1 和 1 之间的数字，用于衡量两个变量之间关系的强度和方向。

## Popular DR Methods
## PCA
<img width="713" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/b8b27183-bb03-403f-92f2-ee7188710ac4">

<img width="697" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/3b9b7cca-eec5-4e15-94c5-69efccc48654">

<img width="410" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/bd79369d-f615-4b77-a2d3-ba33daa7fc31">

<img width="684" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/6f53c8d7-4cb3-4f88-ac25-f6ae86c3c7ae">
- 两种方法的 PCA 计算结果相同。
o 最大方差
o 平均投影误差最小
- PCA 需要计算
o 观察变量的平均值
o 观察变量的协方差
o 计算协方差矩阵的特征值/特征向量

<img width="566" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/08075a73-2cf2-4c9d-81eb-4ad4e0503e4c">

<img width="627" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/52efa441-ba01-46ad-b484-a2c9ad0f6f32">

