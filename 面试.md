# 面试
## 回归模型和分类模型常用损失函数有哪些？各有什么优缺点
回归模型常用的损失函数有：

0-1损失函数： 

$$ L(f(x),y) = \begin{cases} 1, & y \neq f(x) \ 0, & y = f(x) \end{cases} $$

绝对损失函数：异常点多的情况下鲁棒性好；但不方便求导

$$ L(f(x), y) = |f(x)-y| $$

平方损失函数：求导方便，能够用梯度下降法优化；对异常值敏感 

$$ L(f(x),y) = (f(x)-y)^2 $$

对数损失函数/对数似然损失函数： 

$$ L(P(Y|X),Y) = -{\rm log} P(Y|X) $$

Huber 损失函数：结合了绝对损失函数和平方损失函数的优点；缺点是需要调整超参数 

$$ L_{Huber}(f, y) = \begin{cases} (f-y)^2 & |f-y| \leq \delta \ 2 \delta |f-y| - \delta^2 & |f-y| > \delta \end{cases} $$

Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微） 

$$ L(f,y) = \log \cosh(f-y) $$

## 1-4 什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？
经验风险（经验损失）：模型关于训练数据集的平均损失

$$ R_{\rm emp}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)) 

结构风险：是在经验风险上加上表示模型复杂度的正则化项 

$$ R_{\rm srm}(f) = \frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f) 

经验风险最小化的策略认为，经验风险最小的模型是最优的模型。

结构风险最小化是为了防止过拟合而提出的，结构风险最小化等价于正则化。结构风险最小化的策略认为结构风险最小的模型是最优的模型。

## 如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？

如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？

<img width="1206" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/c2aea65f-e4be-4608-9792-c70002580387">
 <img width="1206" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/f9f05bf5-b023-4705-bde0-5f8036188206">
<img width="673" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/64d504b3-fdcb-409a-bd00-242d712cde5c">


 ## 决策树
 ### 讲解完成的决策树的建树过程

 自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。

### 2-8-2 你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？
熵（entropy）是表示随机变量不确定性的度量， 是一个取有限个值的离散随机变量，其概率分布为
 
 $$ P(X = x_i) = p_i, \ i=1,2,\cdots,n $$ 
 
 则随机变量的熵定义为
 
 $$ H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i $$ 
 
 熵越大，随机变量的不确定性就越大。

而熵其实表示的是一个系统的平均信息量。自信息量是用来描述某一条信息的大小 

$$ I = - {\rm log} \ p_i $$ 

通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。而通常我们衡量整个系统的信息量，系统存在多个事件  ，每个事件的概率分布

$P={p_1,\cdots,p_n}$ 

，熵是整个系统的平均信息量 

## 联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？

联合熵：将一维随机变量分布推广到多维随机变量分布

$$ H(X,Y) = -\sum\limits_{x,y} p(x,y)\ {\rm log}\ p(x,y) $$ 

条件熵：某个特征A对于数据集D的经验条件熵为 

$$ H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \ = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log } \frac{|D_{ik}|}{|D_i|} \rgroup $$ 

信息增益： 
 定义为数据集D的经验熵 
 与特征A给定条件下D的经验条件熵 
 的差
 
 $$ g(D,A) = H(D) - H(D|A) $$

信息增益比：特征A对于数据集D 的信息增益比定义为 

$$ g_R(D|A) = \frac{g(D|A)}{H_A(D)} $$ 其中 $$ H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log } \frac{|D_i|}{|D|} $$ 

为数据集D关于A的取值熵；n为特征A在D上的取值数目；

Gini系数：描述数据的不确定性。数据集D的Gini系数为

$$ {\rm Gini}(D) = 1 - \sum_{k=1}^{K }(\frac{|C_k|}{|D|})^2 $$ 

其中是D中第k类的样本子集，K是类的个数。例如二分类问题，K=2。基尼系数越大，样本集合的不确定性也就越大，这一点与熵相似。基尼系数Gini(D,A)表示经A=a分割后集合D的不确定性。

交叉熵：刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般p(x)为真实分布，q(x)为预测分布

交叉熵不对称。交叉熵越小，概率分布越接近

$$ H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x) $$ KL散度/相对熵： $$ D_{K L}(p | q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right) $$ 

n表示事件可能发生的情况总数，KL散度的值越小表示两个分布越接近。

$$ D_{KL}(p||q) = H(p,q) - H(p) $$

机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的后半部分是一个常量，所以我们常常将前半部分的交叉熵作为损失函数，其实二者是一样的。

