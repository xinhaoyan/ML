# 面试
## 回归模型和分类模型常用损失函数有哪些？各有什么优缺点
回归模型常用的损失函数有：

0-1损失函数： 

$$ L(f(x),y) = \begin{cases} 1, & y \neq f(x) \ 0, & y = f(x) \end{cases} $$

绝对损失函数：异常点多的情况下鲁棒性好；但不方便求导

$$ L(f(x), y) = |f(x)-y| $$

平方损失函数：求导方便，能够用梯度下降法优化；对异常值敏感 

$$ L(f(x),y) = (f(x)-y)^2 $$

对数损失函数/对数似然损失函数： 

$$ L(P(Y|X),Y) = -{\rm log} P(Y|X) $$

Huber 损失函数：结合了绝对损失函数和平方损失函数的优点；缺点是需要调整超参数 

$$ L_{Huber}(f, y) = \begin{cases} (f-y)^2 & |f-y| \leq \delta \ 2 \delta |f-y| - \delta^2 & |f-y| > \delta \end{cases} $$

Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微） 

$$ L(f,y) = \log \cosh(f-y) $$

## 1-4 什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？
经验风险（经验损失）：模型关于训练数据集的平均损失

$$ R_{\rm emp}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)) 

结构风险：是在经验风险上加上表示模型复杂度的正则化项 

$$ R_{\rm srm}(f) = \frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f) 

经验风险最小化的策略认为，经验风险最小的模型是最优的模型。

结构风险最小化是为了防止过拟合而提出的，结构风险最小化等价于正则化。结构风险最小化的策略认为结构风险最小的模型是最优的模型。

## 如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？

如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？

<img width="1206" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/c2aea65f-e4be-4608-9792-c70002580387">
<img width="1206" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/f9f05bf5-b023-4705-bde0-5f8036188206">
<img width="673" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/64d504b3-fdcb-409a-bd00-242d712cde5c">


 ## 决策树
 ### 讲解完成的决策树的建树过程

 自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。

### 2-8-2 你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？
熵（entropy）是表示随机变量不确定性的度量， 是一个取有限个值的离散随机变量，其概率分布为
 
 $$ P(X = x_i) = p_i, \ i=1,2,\cdots,n $$ 
 
 则随机变量的熵定义为
 
 $$ H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i $$ 
 
 熵越大，随机变量的不确定性就越大。

而熵其实表示的是一个系统的平均信息量。自信息量是用来描述某一条信息的大小 

$$ I = - {\rm log} \ p_i $$ 

通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。而通常我们衡量整个系统的信息量，系统存在多个事件  ，每个事件的概率分布

$P={p_1,\cdots,p_n}$ 

，熵是整个系统的平均信息量 

## 联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？

联合熵：将一维随机变量分布推广到多维随机变量分布

$$ H(X,Y) = -\sum\limits_{x,y} p(x,y)\ {\rm log}\ p(x,y) $$ 

条件熵：某个特征A对于数据集D的经验条件熵为 

$$ H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \ = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log } \frac{|D_{ik}|}{|D_i|} \rgroup $$ 

信息增益： 
 定义为数据集D的经验熵与特征A给定条件下D的经验条件熵的差
 
 $$ g(D,A) = H(D) - H(D|A) $$

信息增益比：特征A对于数据集D 的信息增益比定义为 

$$ g_R(D|A) = \frac{g(D|A)}{H_A(D)} $$ 其中 $$ H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log } \frac{|D_i|}{|D|} $$ 

为数据集D关于A的取值熵；n为特征A在D上的取值数目；

Gini系数：描述数据的不确定性。数据集D的Gini系数为

$$ {\rm Gini}(D) = 1 - \sum_{k=1}^{K }(\frac{|C_k|}{|D|})^2 $$ 

其中是D中第k类的样本子集，K是类的个数。例如二分类问题，K=2。基尼系数越大，样本集合的不确定性也就越大，这一点与熵相似。基尼系数Gini(D,A)表示经A=a分割后集合D的不确定性。

交叉熵：刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般p(x)为真实分布，q(x)为预测分布

交叉熵不对称。交叉熵越小，概率分布越接近

$$ H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x) $$ KL散度/相对熵： $$ D_{K L}(p | q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right) $$ 

n表示事件可能发生的情况总数，KL散度的值越小表示两个分布越接近。

$$ D_{KL}(p||q) = H(p,q) - H(p) $$

机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的后半部分是一个常量，所以我们常常将前半部分的交叉熵作为损失函数，其实二者是一样的。

<img width="771" alt="image" src="https://github.com/xinhaoyan/ML/assets/111496997/a514b7ee-7faa-4e6c-b6a4-d9266b55c847">

#### ID3 最大信息增益
信息增益 
 定义为数据集D的经验熵 
 与特征A给定条件下D的经验条件熵 
 的差 $$ g(D,A) = H(D) - H(D|A) $$ 选择 
 最大的特征，所有样本根据此特征，划分到不同的节点上。在经验熵不为0的节点中继续生长。ID3算法只有树的生成，容易产生过拟合。

#### C4.5 最大信息增益比
因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，使用信息增益比来选择最优划分属性。

#### CART 基尼指数
基尼系数Gini（D）用来表示集合D的不确定性。CART在每一次迭代中选择划分后基尼指数最小的特征及其对应的切分点进行分类。CART是一颗二叉树，每次将数据按特征A的区分分成两份，分别进入左右子树。

## 决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么

通过剪枝防止过拟合。

预剪枝是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别

预剪枝对于何时停止决策树的生长：

（1）当树达到一定深度

（2）当到达当前节点的样本数量小于某个阈值

（3）计算每次分裂对测试集的准确度提升，小于某个阈值时停止

后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶子节点进行考察，若该节点对应的子树替换成叶子结点能带来泛化性能提升，则将该子树替换为叶子节点。

### 2-8-5 决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么
通过剪枝防止过拟合。

预剪枝是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别

预剪枝对于何时停止决策树的生长：

（1）当树达到一定深度

（2）当到达当前节点的样本数量小于某个阈值

（3）计算每次分裂对测试集的准确度提升，小于某个阈值时停止

后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶子节点进行考察，若该节点对应的子树替换成叶子结点能带来泛化性能提升，则将该子树替换为叶子节点。

## 随机森林（RF）
### 可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？
随机森林属于bagging类的集成学习方法，主要好处是减小集成后分类器的方差，比基分类器的方差小。所以Bagging所采用的的基分类器最好是本身对样本分布较为敏感（不稳定分类器），这样bagging才能体现效果。而线性分类器和KNN属于较为稳定的分类器，本身方差不大，所以将他们作为基分类器使用bagging不能再原基分类器的基础上获得更好的表现。相反地，可能因为bagging的采样而使得训练中难以收敛从而增大集成分类器的偏差。

## k-means
kmean的总体特点基于划分的聚类方法类别k事先指定 以欧式距离平方表示样本之间的距离 以中心或样本的均值表示类别 以样本和其所属类的中心之间的距离总和为最优化的目标函数 得到的类别是平坦的、非层次化的 算法是迭代算法，不能保证全局最优

## 简述kmeans建模过程？

kmeans聚类是基于样本集合划分的聚类算法

（1）首先随机选择k个样本点作为初始聚类中心

（2）计算每个样本到类中心的距离，将样本逐个指派到与其最近的类中，得到一个聚类结果

（3）更新每个类的样本的均值，作为新的中心

（4）重复以上步骤，知道划分不再改变，收敛为止

kmeans的算法复杂度是 $O(mnk)$ ，其中m是样本位数，n是样本个数，k是类别个数。比层次聚类复杂度低

##  Kmeans损失函数是如何定义？

样本与所属类的中心之间的距离的总和为损失函数 

$$ W(C) = \sum_{l=1}^k \sum_{C(i)=l} {||x_i - \overline{x}_l||} $$ 

其中是第 个类的均值或中心。相似的样本被聚到同类时，损失函数值最小。但是这是一个组合优化问题，n个样本分到k个类，可能的分法数目是指数级的，NP难问题。采样迭代的方法求解


## 2-11-3 你是如何选择初始类族的中心点？

初始类中心点的选择

（1）可以用层次聚类对样本进行聚类，得到k个类时停止。然后从每个类中选取一个与中心距离最近的点 类别数k的选择

k值需要预先指定，而在实际应用中最优的k值是不知道的。解决这个问题的一个方法是尝试用不同的k值聚类，检验各自得到聚类结果的质量，推测最优的k值

（1）一般地，类别数变小时，平均直径会增加；类别数变大超过某个值后，平均直径会不变；而这个值是最优的k值。实验时可以采用二分查找，快速找到最优的k值

## PCA降维

 2-12-1 为什么要对数据进行降维？它能解决什么问题？

高维（多变量）数据，很难观察变量的样本区分能力，也很难观察样本之间的关系。降维是将样本集合中的样本从高维空间转换到低维空间。假设样本原本存在于低维空间，或近似存在与低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。降维有线性降维和非线性降维。

维度灾难

 2-12-2 你是如何理解维度灾难？

特征数量超过一定值的时候，分类器的效果反而下降。原因：特征数过多，过拟合

 2-12-3 PCA主成分分析思想是什么？

变量之间可能存在相关性，以致增加了分析的难度。考虑用少数不想管的变量来替代相关的变量，用来表示数据，并且要求能保留数据中的大部分信息。

PCA利用正交变换把线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。PCA属于降维方法。

 2-12-4 如何定义主成分？

协方差矩阵的特征向量

 2-12-5 如何设计目标函数使得降维达到提取主成分的目的？

PCA目标函数：最大化投影方差。因为方差表示新变量的信息量大小。

 2-12-6 PCA有哪些局限性？如何优化

（1）无法进行非线性降维

​ 通过核映射对PCA进行扩展得到核主成分分析（KPCA）

​ 通过流形映射的降维方法，比如等距映射、局部线性嵌入（LLE）、拉普拉斯特征映射等

（2）无监督的，算法没有考虑数据的标签，只是把把元数据映射到方差比较大的方向 有监督的降维方法：线性判别分析 LDA

 2-12-7 线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？

PCA选择的是投影后数据方差最大的方向。由于PCA是无监督的，因此假设方差越大，信息量越多，用主成分来表示原始数据可以去除冗余的维度，达到降维。

LDA选择的是投影后类内方差最小，类间方差最大的方向。其用到了类别标签信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向投影后，不同类别尽可能区分开。
